{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Sampling Methods to solve a Linear Regression Problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Metropolis Hastings Sampling\n",
    "\n",
    "## - Hamiltonian Monte Carlo\n",
    "\n",
    "## - Automatic Relevance Detection with HMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst regression problems can be solved in closed form, we will demonstrate sampling methods to sample from the joint posterior of our hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy              as np\n",
    "import pandas             as pd\n",
    "import scipy.stats        as stats\n",
    "import scipy.spatial      as spatial\n",
    "import seaborn            as seabornInstance \n",
    "import matplotlib.pyplot  as plt\n",
    "import statsmodels.api    as sm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn              import metrics\n",
    "from tqdm.notebook        import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ML_implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will solve a regression problem where we assume a mean zero Gaussian prior on the weights with precision alpha and a mean zero Gaussian prior on the noise with precision beta.\n",
    "\n",
    "We will work on the Energy Efficiency dataset which has 8 predictors + intercept/bias.\n",
    "\n",
    "Preprocessing has been performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ee-train.csv\", \"rb\") as csvfile:\n",
    "    train = pd.read_csv(\"ee-train.csv\",low_memory=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ee-test.csv\", \"rb\") as csvfile:\n",
    "    test = pd.read_csv(\"ee-test.csv\",low_memory=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_means = np.mean(train, axis = 0)\n",
    "train_std   = np.std(train,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_norm = (train - train_means)/train_std\n",
    "train_norm.loc[:,'Heating Load'] = train.loc[:,'Heating Load']\n",
    "train_norm.insert(0,'Constant',1)\n",
    "n_features = train_norm.shape[1]\n",
    "test_norm = (test - train_means)/train_std\n",
    "test_norm.loc[:,'Heating Load'] = test.loc[:,'Heating Load']\n",
    "test_norm.insert(0,'Constant',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_norm.iloc[:,:n_features-1]\n",
    "y_train = train_norm.iloc[:,n_features-1]\n",
    "x_test  = test_norm.iloc[:,:n_features-1]\n",
    "y_test  = test_norm.iloc[:,n_features-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metropolis - Hastings Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_star = lambda x: stats.multivariate_normal.rvs(size=1, mean=x, cov = 0.01*np.eye(len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_star(x, X, Y):\n",
    "    \n",
    "    alpha     = x[0]\n",
    "    \n",
    "    beta      = x[1]\n",
    "    \n",
    "    w         = x[2:]\n",
    "    \n",
    "    alpha     = np.exp(alpha)\n",
    "    \n",
    "    beta      = np.exp(beta)\n",
    "    \n",
    "    n,m       = X.shape\n",
    "    \n",
    "    first     = -(n/2)*np.log(2*np.pi/beta)  - (beta/2)*((Y - X @ w)**2).sum()\n",
    "    \n",
    "    second    = -(m/2)*np.log(2*np.pi/alpha)  - (alpha/2)*(w**2).sum()\n",
    "    \n",
    "    pdf       = first + second\n",
    "    \n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh        = sampling.MH(p_star, q_star, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples   = mh.sample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = mh.estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha   = np.exp(estimates[0])\n",
    "beta    = np.exp(estimates[1])\n",
    "weights = estimates[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve a RMSE on the test of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.889639117679848"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean(np.square(x_test @ weights - y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamiltonian MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_func(x, x_train, y_train):\n",
    "    \n",
    "    alpha     = np.exp(x[0])\n",
    "    \n",
    "    beta      = np.exp(x[1])\n",
    "    \n",
    "    w         = np.array(x[2:])\n",
    "\n",
    "    n,m       = x_train.shape\n",
    "    \n",
    "    mat       = y_train.T @ y_train - w.T @ x_train.T @ y_train - y_train.T @ x_train @ w + w.T @ x_train.T @ x_train @ w\n",
    "    \n",
    "    pdf       = -(n/2)*np.log((2*np.pi)/beta)  - (beta/2)*(mat) \\\n",
    "                +(m/2)*np.log(alpha/(2*np.pi)) - (alpha/2)*(w.T @ w)\n",
    "    \n",
    "    return -pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_grad(x, x_train, y_train):\n",
    "    \n",
    "    alpha = np.exp(x[0])\n",
    "    \n",
    "    beta  = np.exp(x[1])\n",
    "    \n",
    "    w     = np.array(x[2:])\n",
    "    \n",
    "    n,m   = x_train.shape\n",
    "    \n",
    "    g = np.empty(11)\n",
    "    \n",
    "    g[0] = 0.5*m - 0.5*alpha*(w.T @ w)\n",
    "    \n",
    "    g[1] = 0.5*n - 0.5*beta*((y_train - x_train @ w)**2).sum()\n",
    "    \n",
    "    g[2:] = -alpha*w - 0.5*beta*(-(x_train.T @ y_train) \\\n",
    "                                 - (y_train.T @ x_train) \\\n",
    "                                 + 2*(x_train.T @ x_train @ w))\n",
    "    \n",
    "    return -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmc = sampling.Hamiltonian(e_func, e_grad, 11, epsilon0 = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03dd935f3384c7f9d1bbbe40753df3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../ML_implementations/sampling/sampling.py:173: RuntimeWarning: overflow encountered in exp\n",
      "  if np.random.uniform() < np.exp(threshold):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8bdfafccd351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Data Science MSc/ML_implementations/sampling/sampling.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, x_train, y_train)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                     \u001b[0mp\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Make a full step for the momentum, except at end of trajectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;31m# Make a half step for momentum at the end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-aedb4cefdfa3>\u001b[0m in \u001b[0;36me_grad\u001b[0;34m(x, x_train, y_train)\u001b[0m\n\u001b[1;32m     17\u001b[0m     g[2:] = -alpha*w - 0.5*beta*(-(x_train.T @ y_train) \\\n\u001b[1;32m     18\u001b[0m                                  \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                                  + 2*(x_train.T @ x_train @ w))\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(self, copy, *args)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# construct the args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m         \u001b[0mdtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_homogeneous_type\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtypes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m             \u001b[0;31m# We have EAs with the same dtype. We can preserve that dtype in transpose.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdtypes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5547\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5549\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_to_dict_of_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "samples = hmc.sample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha   = np.exp(samples[-1,0])\n",
    "beta    = np.exp(samples[-1,1])\n",
    "weights = samples[-1,2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve a RMSE on the test of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.mean(np.square(x_test @ weights - y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Relevance Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change our prior on the weights to a Gaussian prior for each weight with different precision hyper-parametrs. We can then sample from this larger joint posterior to determine which features are relevant. High precision indicated low variance and therefore weights likely close to zero (mean zero prior assumed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_func(x, x_train, y_train):\n",
    "    \n",
    "    alphas     = np.exp(x[0:9])\n",
    "    \n",
    "    beta      = np.exp(x[9])\n",
    "    \n",
    "    w         = np.array(x[10:])\n",
    "\n",
    "    n,m       = x_train.shape\n",
    "    \n",
    "    mat       = y_train.T @ y_train - w.T @ x_train.T @ y_train \\\n",
    "                - y_train.T @ x_train @ w + w.T @ x_train.T @ x_train @ w\n",
    "    \n",
    "    pdf       = -(n/2)*np.log((2*np.pi)/beta)   - (beta/2)*(mat) \\\n",
    "                + 0.5*np.sum(np.log(alphas)) - 0.5*np.sum(alphas*(w**2)) - (m/2)*np.log(2*np.pi)\n",
    "    \n",
    "    return -pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_grad(x, x_train, y_train):\n",
    "    \n",
    "    alphas = np.exp(x[0:9])\n",
    "    \n",
    "    beta  = np.exp(x[9])\n",
    "    \n",
    "    w     = np.array(x[10:])\n",
    "    \n",
    "    n,m   = x_train.shape\n",
    "    \n",
    "    g     = np.empty(19)\n",
    "    \n",
    "    g[0:9]  = (1/2)*(1 - alphas*(w**2))\n",
    "    \n",
    "    g[9]    = 0.5*n - 0.5*beta*((y_train - x_train @ w)**2).sum()\n",
    "    \n",
    "    g[10:]  = -alphas*w - 0.5*beta*(-(x_train.T @ y_train) \\\n",
    "                                 - (y_train.T @ x_train) \\\n",
    "                                 + 2*(x_train.T @ x_train @ w))\n",
    "    \n",
    "    return -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmc_ard = sampling.Hamiltonian(e_func, e_grad, 19, epsilon0 = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ard_samples = hmc_ard.sample(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha   = np.exp(ard_samples[-1,0:9])\n",
    "beta    = np.exp(ard_samples[-1,9])\n",
    "weights = ard_samples[-1,10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve a RMSE on the test of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.mean(np.square(x_test @ weights - y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With alphas of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And therefore features variances of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/alpha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
